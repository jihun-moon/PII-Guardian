# ğŸ•µï¸ (ë´‡ 1) 'ì‹ ì…' ë´‡. 'ì˜ì‹¬' ë‚´ì—­ ìˆ˜ì§‘ -> detected_leaks.csv
# ----------------------------------------------------
# 1. í…ìŠ¤íŠ¸/ì´ë¯¸ì§€(OCR)ì—ì„œ 'ì˜ì‹¬' PIIë¥¼ 1ì°¨ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
# 2. (ì„ íƒ) GitHub APIì—ì„œ 'ì˜ì‹¬' PIIë¥¼ 1ì°¨ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
# ----------------------------------------------------

import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import os
import time
from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification
from urllib.parse import urljoin

# ìš°ë¦¬ í—¬í¼ ë° ì„¤ì • íŒŒì¼ ì„í¬íŠ¸
import config
import ocr_helper 

# --- 1. ì„¤ì •ê°’ ---
# (ë°ì´í„° ì €ì¥ íŒŒì¼)
CSV_FILE = 'detected_leaks.csv'
FEEDBACK_FILE = 'feedback_data.csv' # ğŸ§‘â€ğŸ« (ë´‡ 2)ì˜ 'ì •ë‹µì§€'
# (NER ëª¨ë¸ ê²½ë¡œ)
MODEL_PATH = 'my-ner-model' # ğŸ“ (ë´‡ 3)ì´ í›ˆë ¨ì‹œí‚¬ ë‡Œ
BASE_MODEL = 'klue/roberta-base-ner' # ğŸ§  ê¸°ë³¸ ë‡Œ (Hugging Face)

# (1ì°¨ íƒì§€ìš© ì •ê·œì‹ íŒ¨í„´)
REGEX_PATTERNS = {
    'EMAIL': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
    'PHONE': r'\b010[-.\s]?\d{4}[-.\s]?\d{4}\b',
    # (íŒ¨í„´ ì¶”ê°€ ê°€ëŠ¥)
    # 'API_KEY': r'sk_[a-zA-Z0-9]{32,}' 
}

# (í¬ë¡¤ë§í•  ëŒ€ìƒ)
# ğŸš¨ (ìˆ˜ì •) ì‚¬ìš©ì ì´ë¦„ì„ 'jihun0948'ì—ì„œ 'jihun-moon'ìœ¼ë¡œ ë°”ë¡œì¡ì•˜ìŠµë‹ˆë‹¤.
TEST_URLS = [
    'https://jihun-moon.github.io/PII-Guardian/test_site/index.html',
    'https://jihun-moon.github.io/PII-Guardian/test_site/page_with_image.html'
]

# (ê¹ƒí—ˆë¸Œ ê²€ìƒ‰ì–´ - ì£¼ì„ ì²˜ë¦¬ë¨)
GITHUB_QUERIES = [
    '"ncp_api_key"',     # NCP API í‚¤
    '"IMë±…í¬" "ë¹„ë°€ë²ˆí˜¸"',
]

# --- 2. ë´‡ì˜ 'ë‡Œ' (AI ëª¨ë¸) ë¡œë“œ (âœ¨ ìµœì¢… ìˆ˜ì •) ---
def load_ner_pipeline():
    """ë´‡ì˜ 'ë‡Œ'(NER ëª¨ë¸)ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    
    # (âœ¨ í•µì‹¬ ìˆ˜ì •)
    # Crontab í™˜ê²½ ë¬¸ì œë¥¼ íšŒí”¼í•˜ê¸° ìœ„í•´, deploy.ymlì´ ìƒì„±í•œ
    # í† í° 'íŒŒì¼'ì„ ì§ì ‘ ì½ì–´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    token_file_path = "/root/.cache/huggingface/token"
    hf_token = None
    if os.path.exists(token_file_path):
        try:
            with open(token_file_path, 'r') as f:
                hf_token = f.read().strip()
            if hf_token:
                 print("âœ… Hugging Face í† í° íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ì½ì—ˆìŠµë‹ˆë‹¤.")
            else:
                 print("âš ï¸ [ê²½ê³ ] /root/.cache/huggingface/token íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ [ê²½ê³ ] Hugging Face í† í° íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    else:
        print("âš ï¸ [ê²½ê³ ] Hugging Face í† í° íŒŒì¼(/root/.cache/huggingface/token)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # (ì°¨ì„ ì±…) íŒŒì¼ì´ ì—†ì„ ê²½ìš°, config.pyì—ì„œë„ ì‹œë„
    if not hf_token:
        hf_token = getattr(config, 'HF_TOKEN', None)
        if hf_token:
             print("âœ… config.pyì—ì„œ HF_TOKENì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

    try:
        # 1ìˆœìœ„: ìš°ë¦¬ê°€ í•™ìŠµì‹œí‚¨ 'ê²½ë ¥ì§' ë‡Œ(my-ner-model)ë¥¼ ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, token=hf_token)
        model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH, token=hf_token)
        print(f"âœ… 'ê²½ë ¥ì§' AI ë‡Œ({MODEL_PATH}) ë¡œë“œ ì„±ê³µ!")
    except OSError: 
        # 2ìˆœìœ„: 1ìˆœìœ„ê°€ ì‹¤íŒ¨í•˜ë©´ 'ì‹ ì…' ë‡Œ(klue/roberta)ë¥¼ ë¡œë“œ
        print(f"âš ï¸ 'ê²½ë ¥ì§' AI ë‡Œ({MODEL_PATH})ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'ì‹ ì…' ë‡Œ({BASE_MODEL})ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
        
        # (âœ¨ í•µì‹¬ 3) 'ì‹ ì…' ë‡Œ ë¡œë“œ ì‹œ, ì¸ì¦ì„ ìœ„í•´ í† í°ì„ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, token=hf_token)
        model = AutoModelForTokenClassification.from_pretrained(BASE_MODEL, token=hf_token)
        
    # AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ì‰½ê²Œ 'íŒŒì´í”„ë¼ì¸'ìœ¼ë¡œ ë§Œë“¦
    ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, device=-1, aggregation_strategy="simple")
    return ner_pipeline

# --- 3. ìœ ì¶œ íƒì§€ í•¨ìˆ˜ (í…ìŠ¤íŠ¸ìš©) ---
def find_leaks_in_text(text, ner_pipeline):
# ... (ì´í•˜ ì½”ë“œëŠ” ì´ì „ê³¼ ë™ì¼) ...
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ RegExì™€ NERë¡œ PIIë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    leaks = []
    
    # (ë¬¸ë§¥ ì €ì¥ì„ ìœ„í•´ í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ)
    context_preview = text.strip().replace('\n', ' ').replace('\r', ' ')[0:300]
    
    # 1. ì •ê·œì‹(RegEx)ìœ¼ë¡œ ë¨¼ì € íƒì§€
    for pii_type, pattern in REGEX_PATTERNS.items():
        for match in re.finditer(pattern, text):
            leaks.append({
                'type': pii_type,
                'content': match.group(0),
                'context': context_preview
            })
            
    # 2. AI(NER)ë¡œ ì¶”ê°€ íƒì§€ (ì˜ˆ: ì‚¬ëŒ ì´ë¦„)
    try:
        # (ê°œì„ ) í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ NERì´ ì˜¤ë¥˜ë¥¼ ë‚¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ 512ìë¡œ ì œí•œ
        ner_results = ner_pipeline(text[:512]) 
        for entity in ner_results:
            # klue/roberta-base-nerëŠ” 'PS'(ì‚¬ëŒì´ë¦„)ì„ íƒì§€
            if entity['entity_group'] == 'PS':
                leaks.append({
                    'type': 'PERSON (AI)',
                    'content': entity['word'],
                    'context': context_preview
                })
    except Exception as e:
        print(f"âŒ [AI ë¶„ì„ ì—ëŸ¬] {e}")
            
    return leaks

# --- 4. í¬ë¡¤ë§ í•¨ìˆ˜ (í…ŒìŠ¤íŠ¸ ì‚¬ì´íŠ¸ìš©) ---
def crawl_test_site(url, ner_pipeline):
    """(ê¸°ëŠ¥ 1) í•˜ë‚˜ì˜ 'í…ŒìŠ¤íŠ¸ URL'ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
    print(f"ğŸ•µï¸ [í…ŒìŠ¤íŠ¸ ì‚¬ì´íŠ¸] í¬ë¡¤ë§ ì‹œì‘: {url}")
    leaks_found = []
    try:
        response = requests.get(url, timeout=10)
        response.encoding = 'utf-8' 
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # (ê°œì„ ) bodyê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„
        if not soup.body:
            return []
            
        page_text = soup.body.get_text(separator=' ')
        
        # 4-1. í…ìŠ¤íŠ¸ì—ì„œ ìœ ì¶œ íƒì§€
        leaks_found.extend(find_leaks_in_text(page_text, ner_pipeline))
        
        # 4-2. ì´ë¯¸ì§€(OCR)ì—ì„œ ìœ ì¶œ íƒì§€
        images = soup.find_all('img')
        for img in images:
            try:
                img_url = img.get('src') # .get()ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì ‘ê·¼
                if not img_url:
                    continue
                    
                # (ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜)
                if not img_url.startswith('http'):
                    img_url = urljoin(url, img_url)
                
                print(f"ğŸ–¼ï¸  ì´ë¯¸ì§€ ìŠ¤ìº” ì¤‘... {img_url}")
                ocr_text = ocr_helper.get_ocr_text(img_url) # ocr_helper.py í˜¸ì¶œ
                
                if ocr_text:
                    image_leaks = find_leaks_in_text(ocr_text, ner_pipeline)
                    if image_leaks:
                        print(f"ğŸš¨ [OCR íƒì§€!] {img_url} ì—ì„œ {len(image_leaks)}ê±´ ë°œê²¬!")
                        leaks_found.extend(image_leaks)
            except Exception as e:
                print(f"âŒ [ì´ë¯¸ì§€ ì—ëŸ¬] {img.get('src')} ìŠ¤ìº” ì‹¤íŒ¨: {e}")

        return leaks_found
            
    except Exception as e:
        print(f"âŒ [ì—ëŸ¬] {url} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

# --- 5. ê¹ƒí—ˆë¸Œ ê²€ìƒ‰ í•¨ìˆ˜ (ì£¼ì„ ì²˜ë¦¬ë¨) ---
def search_github_api(query, ner_pipeline):
    """(ê¸°ëŠ¥ 2) GitHub APIë¡œ 'ì‹¤ì œ' ì†ŒìŠ¤ ì½”ë“œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    print(f"ğŸ›°ï¸ [GitHub API] ê²€ìƒ‰ ì‹œì‘: {query}")
    
    API_URL = "https://api.github.com/search/code"
    headers = {
        "Authorization": f"token {getattr(config, 'GITHUB_TOKEN', '')}", 
        "Accept": "application/vnd.github.v3.text-match+json" 
    }
    params = {'q': query, 'sort': 'indexed', 'order': 'desc', 'per_page': 10} 
    
    total_leaks = []
    try:
        response = requests.get(API_URL, headers=headers, params=params, timeout=10)
        response.raise_for_status() 
        results = response.json()
        
        if 'items' not in results or not results['items']:
            print("âœ… [GitHub API] íƒì§€ëœ ë‚´ì—­ ì—†ìŒ.")
            return []
            
        for item in results['items']:
            file_url = item['html_url']
            repo_name = item['repository']['full_name']
            
            code_context = ""
            if 'text_matches' in item and item['text_matches']:
                # (ìˆ˜ì •) text_matchesëŠ” ì—¬ëŸ¬ ê°œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëª¨ë‘ í•©ì¹©ë‹ˆë‹¤.
                code_context = " ... ".join([match['fragment'] for match in item['text_matches']])
            
            if code_context:
                leaks = find_leaks_in_text(code_context, ner_pipeline)
                for leak in leaks:
                    leak['url'] = file_url 
                    leak['repo'] = repo_name
                total_leaks.extend(leaks)
        
        if total_leaks:
            print(f"ğŸš¨ [GitHub íƒì§€!] ì´ {len(total_leaks)}ê±´ ë°œê²¬!")
        return total_leaks
        
    except Exception as e:
        print(f"âŒ [GitHub API ì—ëŸ¬] {e}")
        return []

# --- 6. CSV ì €ì¥ í•¨ìˆ˜ (âœ¨ ë¡œì§ ëŒ€í­ ê°œì„ ) ---
def save_to_csv(all_leaks):
    """íƒì§€ëœ ëª¨ë“  ë‚´ì—­ì„ 'ì˜ì‹¬' ëª©ë¡(CSV)ì— 'ì¶”ê°€'í•©ë‹ˆë‹¤."""
    if not all_leaks:
        return
            
    new_df = pd.DataFrame(all_leaks)
    
    # (âœ¨ ê°œì„  1) ì´ë¯¸ 'ì •ë‹µì§€'ì— ìˆëŠ” ë‚´ì—­ì€ ì œì™¸í•©ë‹ˆë‹¤.
    try:
        if os.path.exists(FEEDBACK_FILE):
            try:
                feedback_df = pd.read_csv(FEEDBACK_FILE)
            except pd.errors.EmptyDataError:
                feedback_df = pd.DataFrame(columns=['content', 'url']) # ë¹ˆ DataFrame

            if not feedback_df.empty:
                # 'ì •ë‹µì§€'ì— ìˆëŠ” (content, url) ìŒì„ ë§Œë“­ë‹ˆë‹¤.
                # (urlì´ ì—†ëŠ” 'test-site'ì˜ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ fillna ì‚¬ìš©)
                feedback_df['url'] = feedback_df['url'].fillna('test-site-url') # ì„ì‹œ ê°’
                new_df['url'] = new_df['url'].fillna('test-site-url') # ì„ì‹œ ê°’
                
                feedback_keys = set(zip(feedback_df['content'], feedback_df['url']))
                
                # (content, url)ì´ 'ì •ë‹µì§€'ì— ì—†ëŠ” ê²ƒë§Œ í•„í„°ë§
                is_new = new_df.apply(lambda row: (row['content'], row['url']) not in feedback_keys, axis=1)
                new_df = new_df[is_new]
                
                if len(new_df) == 0:
                    print("âœ… ìƒˆë¡œ ë°œê²¬ëœ 'ì˜ì‹¬' ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤. (ëª¨ë‘ 'ì •ë‹µì§€'ì— ì´ë¯¸ ì¡´ì¬)")
                    return
                else:
                    print(f"âœ¨ 'ì •ë‹µì§€'ì™€ ë¹„êµ í›„, {len(new_df)}ê±´ì˜ 'ì‹ ê·œ' ë‚´ì—­ ë°œê²¬!")

    except Exception as e:
        print(f"âš ï¸ 'ì •ë‹µì§€'({FEEDBACK_FILE}) ë¹„êµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # (âœ¨ ê°œì„  2) 'ì˜ì‹¬' ëª©ë¡(detected_leaks.csv) ë‚´ì˜ ì¤‘ë³µë„ ì œê±°í•©ë‹ˆë‹¤.
    if os.path.exists(CSV_FILE):
        try:
            existing_df = pd.read_csv(CSV_FILE)
            combined_df = pd.concat([existing_df, new_df])
            # 'content'ì™€ 'url'ì´ ëª¨ë‘ ë˜‘ê°™ì€ ì¤‘ë³µì€ ì œê±°
            final_df = combined_df.drop_duplicates(subset=['content', 'url'])
        except pd.errors.EmptyDataError: # íŒŒì¼ì´ ë¹„ì–´ìˆëŠ” ê²½ìš°
            final_df = new_df
    else:
        final_df = new_df
        
    final_df.to_csv(CSV_FILE, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ 'ì˜ì‹¬' ëª©ë¡ ì €ì¥ ì™„ë£Œ: {len(final_df)} ê±´")

# --- 7. ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    print("ğŸ¤– 1. 'ì‹ ì…' ë´‡(Crawler) ì‘ë™ ì‹œì‘...")
    
    print("ğŸ§  ë´‡ì˜ AI ë‡Œ(NER ëª¨ë¸)ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...")
    ner_brain = load_ner_pipeline()
    print("ğŸ§  AI ë‡Œ ë¡œë“œ ì™„ë£Œ.")
    
    total_leaks_found = []
    
    # (í•„ìˆ˜) í…ŒìŠ¤íŠ¸ ì‚¬ì´íŠ¸ í¬ë¡¤ë§
    for url in TEST_URLS:
        leaks = crawl_test_site(url, ner_pipeline)
        for leak in leaks:
            leak['url'] = url 
            leak['repo'] = 'test-site'
        total_leaks_found.extend(leaks)
        
    # (ì„ íƒ) ì‹¤ì œ GitHub API ê²€ìƒ‰
    print("ğŸ›°ï¸ [GitHub API] ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    if not hasattr(config, 'GITHUB_TOKEN') or not config.GITHUB_TOKEN:
        print("âš ï¸ config.pyì— GITHUB_TOKENì´ ì—†ìŠµë‹ˆë‹¤. GitHub ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        for q in GITHUB_QUERIES:
            leaks = search_github_api(q, ner_brain)
            total_leaks_found.extend(leaks)
            time.sleep(5) # (ì¤‘ìš”) API ì œí•œì„ í”¼í•˜ê¸° ìœ„í•´ 5ì´ˆê°„ íœ´ì‹
            
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    if total_leaks_found:
        save_to_csv(total_leaks_found)
    
    print("ğŸ¤– 1. 'ì‹ ì…' ë´‡(Crawler) ì‘ë™ ì™„ë£Œ.")


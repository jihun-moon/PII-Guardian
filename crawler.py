# ğŸ•µï¸ (ë´‡ 1) 'ì‹ ì…' ë´‡. 'ì˜ì‹¬' ë‚´ì—­ ìˆ˜ì§‘ -> detected_leaks.csv
# (v3.1 - Selenium ì œê±°, Requests ë³µê·€, Raw URL ìŠ¤ìº”, ë¬¸ë§¥(Context) ë¡œì§ ìˆ˜ì •)

import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import os
import time
from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification
from urllib.parse import urljoin 
import logging
# (âœ¨ Selenium ê´€ë ¨ ëª¨ë“ˆ ëª¨ë‘ ì‚­ì œ)

# ìš°ë¦¬ í—¬í¼ ë° ì„¤ì • íŒŒì¼ ì„í¬íŠ¸
import config
import ocr_helper # (OCRì€ ì—¬ì „íˆ ë¹„í™œì„±í™”)

# --- 1. ì„¤ì •ê°’ ---
BASE_PATH = "/root/PII-Guardian"
LOG_FILE = os.path.join(BASE_PATH, 'crawler.log')

# (âœ¨ ë¡œê·¸ ì¤‘ë³µ ì œê±°)
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.StreamHandler()])

CSV_FILE = os.path.join(BASE_PATH, 'detected_leaks.csv')
FEEDBACK_FILE = os.path.join(BASE_PATH, 'feedback_data.csv')
MODEL_PATH = os.path.join(BASE_PATH, 'my-ner-model')
BASE_MODEL = 'klue/roberta-base' 

# (âœ¨ v2.21 ì •ê·œì‹)
REGEX_PATTERNS = {
    'EMAIL': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
    'PHONE': r'\b\(?(010)\)?[-.)\s]*\d{3,4}[-.\s]*\d{4}\b',
    'RRN': r'\b\d{6}[- ]*[1-4]\d{6}\b', 
    'CREDIT_CARD': r'\b\d{4}[- ]*\d{4}[- ]*\d{4}[- ]*\d{4}\b', 
    'ACCOUNT_NUM': r'\b\d{3}[- ]*\d{2,6}[- ]*\d{2,7}\b', 
    'API_KEY': r'\b(sk|pk|im-key-prod)-[a-zA-Z0-9_,-]{20,}\b',
    'INTERNAL_IP': r'\b(192\.168\.\d{1,3}\.\d{1,3})\b|\b(10\.\d{1,3}\.\d{1,3}\.\d{1,3})\b',
    'PHONE_GENERAL': r'\b\(?(0[2-9][0-9]?)\)?[-.)\s]*\d{3,4}[-.\s]*\d{4}\b|\b(15\d{2}|16\d{2})[-.\s]*\d{4}\b'
}

# (âœ¨âœ¨âœ¨ í•µì‹¬ ìˆ˜ì •: GitHub 'Raw' URLë¡œ ë³€ê²½ âœ¨âœ¨âœ¨)
# (Seleniumì´ í•„ìš” ì—†ëŠ” 'ì§„ì§œ' ì›ë³¸ íŒŒì¼ ì£¼ì†Œ)
CRAWL_URLS = [
    "http://127.0.0.1:5000/"]

# (âœ¨ Selenium ë“œë¼ì´ë²„ ì„¤ì • í•¨ìˆ˜ ì‚­ì œ)

# --- 2. ë´‡ì˜ 'ë‡Œ' (AI ëª¨ë¸) ë¡œë“œ ---
def load_ner_pipeline():
    """ë´‡ì˜ 'ë‡Œ'(NER ëª¨ë¸)ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    token_file_path = "/root/.cache/huggingface/token"
    hf_token = None
    if os.path.exists(token_file_path):
        try:
            with open(token_file_path, 'r') as f:
                hf_token = f.read().strip()
            if hf_token:
                 logging.info("âœ… Hugging Face í† í° íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ì½ì—ˆìŠµë‹ˆë‹¤.")
            else:
                 logging.warning("âš ï¸ [ê²½ê³ ] /root/.cache/huggingface/token íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logging.warning(f"âš ï¸ [ê²½ê³ ] Hugging Face í† í° íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    else:
        logging.warning("âš ï¸ [ê²½ê³ ] Hugging Face í† í° íŒŒì¼(/root/.cache/huggingface/token)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    if not hf_token:
        hf_token = getattr(config, 'HF_TOKEN', None)
        if hf_token:
             logging.info("âœ… config.pyì—ì„œ HF_TOKENì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    
    if not hf_token:
        logging.error("âŒ [ì¹˜ëª…ì  ì˜¤ë¥˜] Hugging Face í† í°ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None 

    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, token=hf_token)
        model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH, token=hf_token)
        logging.info(f"âœ… 'ê²½ë ¥ì§' AI ë‡Œ({MODEL_PATH}) ë¡œë“œ ì„±ê³µ!")
    except Exception as e: 
        logging.warning(f"âš ï¸ 'ê²½ë ¥ì§' AI ë‡Œ({MODEL_PATH}) ë¡œë“œ ì‹¤íŒ¨. ì›ì¸: {e}")
        logging.info(f"â¡ï¸ 'ì‹ ì…' ë‡Œ({BASE_MODEL})ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
        try:
            tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, token=hf_token)
            model = AutoModelForTokenClassification.from_pretrained(BASE_MODEL, token=hf_token)
        except Exception as e2:
            logging.error(f"âŒ [ì¹˜ëª…ì  ì˜¤ë¥˜] 'ì‹ ì…' ë‡Œ({BASE_MODEL}) ë¡œë“œì—ë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e2}")
            return None
        
    ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, device=-1, aggregation_strategy="simple")
    return ner_pipeline

# --- 3. (âœ¨âœ¨âœ¨ í•µì‹¬ ìˆ˜ì • v3.1: 'ë¬¸ë§¥' ë¡œì§ ìˆ˜ì • âœ¨âœ¨âœ¨) ---
def find_leaks_in_text(text, ner_pipeline):
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ RegExì™€ NERë¡œ PIIë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    leaks = []
    if not text: 
        return leaks
        
    # (âœ¨ ìˆ˜ì •) í˜ì´ì§€ ì „ì²´ 300ìê°€ ì•„ë‹Œ, PII ì£¼ë³€ì˜ ë¬¸ë§¥ì„ ì €ì¥í•©ë‹ˆë‹¤.
    # context_preview = text.strip().replace('\n', ' ').replace('\r', ' ')[0:300] # (ë²„ê·¸ê°€ ìˆë˜ ì½”ë“œ ì‚­ì œ)
    
    for pii_type, pattern in REGEX_PATTERNS.items():
        for match in re.finditer(pattern, text):
            
            # (âœ¨ ì‹ ê·œ) PIIë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì•ë’¤ 150ì, ì´ 300ì ë‚´ì™¸ì˜ ë¬¸ë§¥ì„ ìƒì„±í•©ë‹ˆë‹¤.
            start = max(0, match.start() - 150)
            end = min(len(text), match.end() + 150)
            context_preview = text[start:end].strip().replace('\n', ' ').replace('\r', ' ')
            
            is_duplicate = False
            for existing_leak in leaks:
                if existing_leak['content'] == match.group(0):
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                leaks.append({
                    'type': pii_type.replace('_GENERAL', ''),
                    'content': match.group(0),
                    'context': context_preview # (âœ¨ ì´ì œ ì˜¬ë°”ë¥¸ ë¬¸ë§¥ì´ ì €ì¥ë¨)
                })
            
    try:
        # (âœ¨ ìˆ˜ì •) í˜ì´ì§€ ìƒë‹¨ 512 í† í°ì´ ì•„ë‹Œ, í…ìŠ¤íŠ¸ ì „ì²´ë¥¼ ìŠ¤ìº”í•©ë‹ˆë‹¤.
        ner_results = ner_pipeline(text) 
        
        for entity in ner_results:
            if entity['entity_group'] in ['PS', 'LC', 'OG', 'PII']: 
                
                # (âœ¨ ì‹ ê·œ) NER ê²°ê³¼ì— ëŒ€í•´ì„œë„ PII ì¤‘ì‹¬ì˜ ë¬¸ë§¥ì„ ìƒì„±í•©ë‹ˆë‹¤.
                start = max(0, entity['start'] - 150)
                end = min(len(text), entity['end'] + 150)
                context_preview = text[start:end].strip().replace('\n', ' ').replace('\r', ' ')

                leak_type = entity['entity_group']
                if leak_type == 'PS': leak_type = 'PERSON (AI)'
                if leak_type == 'LC': leak_type = 'LOCATION (AI)'
                if leak_type == 'OG': leak_type = 'ORGANIZATION (AI)'
                if leak_type == 'PII': leak_type = 'PII (Custom AI)'
                
                leaks.append({
                    'type': leak_type,
                    'content': entity['word'],
                    'context': context_preview # (âœ¨ ì´ì œ ì˜¬ë°”ë¥¸ ë¬¸ë§¥ì´ ì €ì¥ë¨)
                })
    except Exception as e:
        logging.error(f"âŒ [AI ë¶„ì„ ì—ëŸ¬] {e}")
            
    return leaks

# --- 4. (âœ¨ ìˆ˜ì •) `requests` ê¸°ë°˜ í¬ë¡¤ë§ í•¨ìˆ˜ (OCR ë¹„í™œì„±í™”) ---
def crawl_web_page(page_url, ner_pipeline):
    """(ê¸°ëŠ¥ 1) `requests`ë¡œ ì •ì  ì›¹í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤. (OCRì€ ë¹„í™œì„±í™”)"""
    logging.info(f"ğŸ•µï¸ [Requests í¬ë¡¤ë§] ì‹œì‘: {page_url}")
    leaks_found = []
    
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}
        response = requests.get(page_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # Raw URLì´ë¯€ë¡œ `response.text`ê°€ ìˆœìˆ˜ HTMLì…ë‹ˆë‹¤.
        html_content = response.text
        
        # (âœ¨ í•µì‹¬) 4-1. HTML ì£¼ì„()ì„ í¬í•¨í•œ ì›ë³¸ í…ìŠ¤íŠ¸ ì „ì²´ ìŠ¤ìº”
        leaks_found.extend(find_leaks_in_text(html_content, ner_pipeline))
        
        # (âœ¨ í•µì‹¬) 4-2. HTML íƒœê·¸ê°€ ì œê±°ëœ, ëˆˆì— ë³´ì´ëŠ” í…ìŠ¤íŠ¸ ìŠ¤ìº”
        soup = BeautifulSoup(html_content, 'html.parser')
        page_text = soup.get_text(separator=' ', strip=True)
        leaks_found.extend(find_leaks_in_text(page_text, ner_pipeline))

        # (OCRì€ ì—¬ì „íˆ ë¹„í™œì„±í™”)
        
        return leaks_found
        
    except Exception as e:
        logging.error(f"âŒ [Requests í¬ë¡¤ë§ ì—ëŸ¬] {page_url} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return []

# --- 5. (ì£¼ì„ ì²˜ë¦¬) ê¹ƒí—ˆë¸Œ ê²€ìƒ‰ í•¨ìˆ˜ ---
# (ìƒëµ)

# --- 6. CSV ì €ì¥ í•¨ìˆ˜ ---
def get_existing_keys(file_path):
    """CSV íŒŒì¼ì—ì„œ (content, url) í‚¤ ì„¸íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not os.path.exists(file_path):
        return set()
    try:
        df = pd.read_csv(file_path)
        if df.empty:
            return set()
        df['url'] = df['url'].fillna('N/A')
        return set(zip(df['content'], df['url']))
    except pd.errors.EmptyDataError:
        return set()
    except Exception as e:
        logging.warning(f"âš ï¸ {file_path} ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return set()

def save_to_csv(all_leaks):
    """íƒì§€ëœ ëª¨ë“  ë‚´ì—­ì„ 'ì˜ì‹¬' ëª©ë¡(CSV)ì— 'ì¶”ê°€'í•©ë‹ˆë‹¤."""
    if not all_leaks:
        return
            
    new_df = pd.DataFrame(all_leaks)
    new_df['url'] = new_df['url'].fillna('N.A')
    
    processed_keys = get_existing_keys(FEEDBACK_FILE)
    pending_keys = get_existing_keys(CSV_FILE)
    all_known_keys = processed_keys.union(pending_keys)
    
    is_truly_new = new_df.apply(lambda row: (row['content'], row['url']) not in all_known_keys, axis=1)
    
    # (âœ¨ ìˆ˜ì •) ì¤‘ë³µ ì œê±° (find_leaks_in_textê°€ 2ë²ˆ í˜¸ì¶œë˜ë¯€ë¡œ)
    final_new_df = new_df[is_truly_new].drop_duplicates(subset=['content', 'url'])

    if final_new_df.empty:
        logging.info("âœ… ìƒˆë¡œ ë°œê²¬ëœ 'ì˜ì‹¬' ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤. (ëª¨ë‘ ê¸°ì¡´ ëª©ë¡ì— ì¡´ì¬)")
        return

    logging.info(f"âœ¨ {len(final_new_df)}ê±´ì˜ 'ì§„ì§œ ì‹ ê·œ' ë‚´ì—­ì„ {CSV_FILE}ì— ì¶”ê°€í•©ë‹ˆë‹¤.")
    final_new_df.to_csv(CSV_FILE, mode='a', header=not os.path.exists(CSV_FILE), index=False, encoding='utf-8-sig')

# --- 7. ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    logging.info("ğŸ¤– 1. 'ì‹ ì…' ë´‡(Crawler) ì‘ë™ ì‹œì‘...")
    
    logging.info("ğŸ§  ë´‡ì˜ AI ë‡Œ(NER ëª¨ë¸)ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...")
    ner_brain = load_ner_pipeline() # <-- ë³€ìˆ˜ëª…ì´ 'ner_brain'
    if ner_brain is None:
        logging.error("âŒ AI ë‡Œ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ 'ì‹ ì…' ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit()
    logging.info("ğŸ§  AI ë‡Œ ë¡œë“œ ì™„ë£Œ.")

    # (âœ¨ Selenium ë“œë¼ì´ë²„ ë¡œë“œ ì½”ë“œ ì‚­ì œ)
    
    total_leaks_found = []
    
    # (âœ¨ `requests` ê¸°ë°˜ í¬ë¡¤ë§ìœ¼ë¡œ ë³€ê²½)
    logging.info(f"ğŸ›°ï¸ [Requests í¬ë¡¤ë§] {len(CRAWL_URLS)}ê°œì˜ URLì„ ìŠ¤ìº”í•©ë‹ˆë‹¤. (OCR ë¹„í™œì„±í™”)")
    for url in CRAWL_URLS:
        leaks = crawl_web_page(url, ner_brain) 
        for leak in leaks:
            leak['url'] = url 
            leak['repo'] = 'web-crawl'
        total_leaks_found.extend(leaks)
        time.sleep(1) # (ì‚¬ì´íŠ¸ ë¶€í•˜ ë°©ì§€)

    # (âœ¨ Selenium ë“œë¼ì´ë²„ ì¢…ë£Œ ì½”ë“œ ì‚­ì œ)

    # (ê¹ƒí—ˆë¸Œ API ê²€ìƒ‰ì€ ì—¬ì „íˆ ì£¼ì„ ì²˜ë¦¬)
            
    # ìµœì¢… ê²°ê³¼ ì €ì¥ (ë¡œê·¸ ì¶”ê°€)
    if total_leaks_found:
        logging.info(f"âœ… ì´ {len(total_leaks_found)}ê°œì˜ PIIë¥¼ íƒì§€í–ˆìŠµë‹ˆë‹¤. CSV ì €ì¥ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        save_to_csv(total_leaks_found)
    else:
        logging.info("âœ… PII íƒì§€ ê²°ê³¼: 0ê±´. CSV íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.") 
    
    logging.info("ğŸ¤– 1. 'ì‹ ì…' ë´‡(Crawler) ì‘ë™ ì™„ë£Œ.")
# ğŸ•µï¸ (ë´‡ 1) 'ì‹ ì…' ë´‡. 'ì˜ì‹¬' ë‚´ì—­ ìˆ˜ì§‘ -> detected_leaks.csv
# (v2.3 - Seleniumìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ / ë™ì  ì‚¬ì´íŠ¸ ìŠ¤ìº” ê°€ëŠ¥)

import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import os
import time
from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification
from urllib.parse import urljoin 
import logging
from selenium import webdriver # (âœ¨ ì‹ ê·œ)
from selenium.webdriver.chrome.options import Options # (âœ¨ ì‹ ê·œ)
from selenium.webdriver.chrome.service import Service # (âœ¨ ì‹ ê·œ)

# ìš°ë¦¬ í—¬í¼ ë° ì„¤ì • íŒŒì¼ ì„í¬íŠ¸
import config
import ocr_helper # (OCRì€ ì—¬ì „íˆ ë¹„í™œì„±í™”)

# --- 1. ì„¤ì •ê°’ ---
BASE_PATH = "/root/PII-Guardian"
LOG_FILE = os.path.join(BASE_PATH, 'crawler.log')

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.FileHandler(LOG_FILE), logging.StreamHandler()])

CSV_FILE = os.path.join(BASE_PATH, 'detected_leaks.csv')
FEEDBACK_FILE = os.path.join(BASE_PATH, 'feedback_data.csv')
MODEL_PATH = os.path.join(BASE_PATH, 'my-ner-model')
BASE_MODEL = 'klue/roberta-base' 

REGEX_PATTERNS = {
    'EMAIL': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
    'PHONE': r'\b010[-.\s]?\d{4}[-.\s]?\d{4}\b',
}

# --- íƒì§€í•  URL (ì´ì œ dcinside.comë„ ê°€ëŠ¥) ---
CRAWL_URLS = [
    "https://www.dcinside.com/", # (ë™ì  ì‚¬ì´íŠ¸)
    "https://comsoft.daegu.ac.kr/comsoft/professor.do", # (ì •ì  ì‚¬ì´íŠ¸)
]

# (âœ¨ ì‹ ê·œ) Selenium ë“œë¼ì´ë²„ ì„¤ì •
def setup_selenium_driver():
    """ì„œë²„ í™˜ê²½ì—ì„œ Selenium Chrome ë“œë¼ì´ë²„ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
    chrome_options = Options()
    # (í•„ìˆ˜) UIê°€ ì—†ëŠ” ë¦¬ëˆ…ìŠ¤ ì„œë²„ì—ì„œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì˜µì…˜
    chrome_options.add_argument("--headless") # (ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•ŠìŒ)
    chrome_options.add_argument("--no-sandbox") # (root ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰ ì‹œ í•„ìˆ˜)
    chrome_options.add_argument("--disable-dev-shm-usage") # (ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ ë°©ì§€)
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")

    # deploy.ymlì´ ì„¤ì¹˜í•œ /usr/local/bin/chromedriverë¥¼ ì‚¬ìš©
    service = Service(executable_path="/usr/local/bin/chromedriver") 
    
    try:
        driver = webdriver.Chrome(service=service, options=chrome_options)
        return driver
    except Exception as e:
        logging.error(f"âŒ [Selenium ì˜¤ë¥˜] Chrome ë“œë¼ì´ë²„ ë¡œë“œ ì‹¤íŒ¨: {e}")
        logging.error("NCP ì„œë²„ì— Chromeê³¼ ChromeDriverê°€ ì •ìƒ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        logging.error("(`deploy.yml`ì´ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ GitHub Actions íƒ­ì—ì„œ í™•ì¸í•˜ì„¸ìš”.)")
        return None

# --- 2. ë´‡ì˜ 'ë‡Œ' (AI ëª¨ë¸) ë¡œë“œ ---
def load_ner_pipeline():
    """ë´‡ì˜ 'ë‡Œ'(NER ëª¨ë¸)ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    # (ë‚´ìš© ë™ì¼ - ìƒëµ)
    token_file_path = "/root/.cache/huggingface/token"
    hf_token = None
    if os.path.exists(token_file_path):
        try:
            with open(token_file_path, 'r') as f:
                hf_token = f.read().strip()
            if hf_token:
                 logging.info("âœ… Hugging Face í† í° íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ì½ì—ˆìŠµë‹ˆë‹¤.")
            else:
                 logging.warning("âš ï¸ [ê²½ê³ ] /root/.cache/huggingface/token íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logging.warning(f"âš ï¸ [ê²½ê³ ] Hugging Face í† í° íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    else:
        logging.warning("âš ï¸ [ê²½ê³ ] Hugging Face í† í° íŒŒì¼(/root/.cache/huggingface/token)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    if not hf_token:
        hf_token = getattr(config, 'HF_TOKEN', None)
        if hf_token:
             logging.info("âœ… config.pyì—ì„œ HF_TOKENì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    
    if not hf_token:
        logging.error("âŒ [ì¹˜ëª…ì  ì˜¤ë¥˜] Hugging Face í† í°ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None 

    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, token=hf_token)
        model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH, token=hf_token)
        logging.info(f"âœ… 'ê²½ë ¥ì§' AI ë‡Œ({MODEL_PATH}) ë¡œë“œ ì„±ê³µ!")
    except Exception as e: 
        logging.warning(f"âš ï¸ 'ê²½ë ¥ì§' AI ë‡Œ({MODEL_PATH}) ë¡œë“œ ì‹¤íŒ¨. ì›ì¸: {e}")
        logging.info(f"â¡ï¸ 'ì‹ ì…' ë‡Œ({BASE_MODEL})ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
        try:
            tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, token=hf_token)
            model = AutoModelForTokenClassification.from_pretrained(BASE_MODEL, token=hf_token)
        except Exception as e2:
            logging.error(f"âŒ [ì¹˜ëª…ì  ì˜¤ë¥˜] 'ì‹ ì…' ë‡Œ({BASE_MODEL}) ë¡œë“œì—ë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e2}")
            return None
        
    ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, device=-1, aggregation_strategy="simple")
    return ner_pipeline

# --- 3. ìœ ì¶œ íƒì§€ í•¨ìˆ˜ (í…ìŠ¤íŠ¸ìš©) ---
def find_leaks_in_text(text, ner_pipeline):
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ RegExì™€ NERë¡œ PIIë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    # (ë‚´ìš© ë™ì¼ - ìƒëµ)
    leaks = []
    if not text: 
        return leaks
        
    context_preview = text.strip().replace('\n', ' ').replace('\r', ' ')[0:300]
    
    for pii_type, pattern in REGEX_PATTERNS.items():
        for match in re.finditer(pattern, text):
            leaks.append({
                'type': pii_type,
                'content': match.group(0),
                'context': context_preview
            })
            
    try:
        ner_results = ner_pipeline(text[:512]) 
        for entity in ner_results:
            if entity['entity_group'] in ['PS', 'LC', 'OG']:
                leak_type = entity['entity_group']
                if leak_type == 'PS': leak_type = 'PERSON (AI)'
                if leak_type == 'LC': leak_type = 'LOCATION (AI)'
                if leak_type == 'OG': leak_type = 'ORGANIZATION (AI)'
                
                leaks.append({
                    'type': leak_type,
                    'content': entity['word'],
                    'context': context_preview
                })
    except Exception as e:
        logging.error(f"âŒ [AI ë¶„ì„ ì—ëŸ¬] {e}")
            
    return leaks

# --- 4. (âœ¨ í•µì‹¬ ìˆ˜ì •) Selenium í¬ë¡¤ë§ í•¨ìˆ˜ (OCR ë¹„í™œì„±í™”) ---
def crawl_web_page(page_url, ner_pipeline, driver):
    """(ê¸°ëŠ¥ 1) Seleniumìœ¼ë¡œ ë™ì  ì›¹í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤. (OCRì€ ë¹„í™œì„±í™”)"""
    logging.info(f"ğŸ•µï¸ [Selenium í¬ë¡¤ë§] ì‹œì‘: {page_url}")
    leaks_found = []
    
    try:
        # 4-1. (âœ¨ ìˆ˜ì •) Seleniumìœ¼ë¡œ í˜ì´ì§€ ë¡œë“œ
        driver.get(page_url)
        # (ì¤‘ìš”) JavaScriptê°€ ë¡œë“œë  ë•Œê¹Œì§€ 5ì´ˆê°„ ëŒ€ê¸°
        time.sleep(5) 
        
        # 4-2. (âœ¨ ìˆ˜ì •) JavaScriptê°€ ì‹¤í–‰ëœ ìµœì¢… HTML ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
        html_content = driver.page_source
        soup = BeautifulSoup(html_content, 'html.parser')
        
        if not soup.body: 
            return []
            
        # 4-3. í˜ì´ì§€ í…ìŠ¤íŠ¸ íƒì§€
        page_text = soup.body.get_text(separator=' ')
        leaks_found.extend(find_leaks_in_text(page_text, ner_pipeline))
        
        # 4-4. (OCRì€ ì—¬ì „íˆ ë¹„í™œì„±í™”)
        # logging.info(f"ğŸ–¼ï¸  (OCR ê¸°ëŠ¥ ë¹„í™œì„±í™”ë¨)")
        
        return leaks_found
        
    except Exception as e:
        logging.error(f"âŒ [Selenium í¬ë¡¤ë§ ì—ëŸ¬] {page_url} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return []

# --- 5. (ì£¼ì„ ì²˜ë¦¬) ê¹ƒí—ˆë¸Œ ê²€ìƒ‰ í•¨ìˆ˜ ---
# (ìƒëµ)

# --- 6. CSV ì €ì¥ í•¨ìˆ˜ ---
def get_existing_keys(file_path):
    """CSV íŒŒì¼ì—ì„œ (content, url) í‚¤ ì„¸íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    # (ë‚´ìš© ë™ì¼ - ìƒëµ)
    if not os.path.exists(file_path):
        return set()
    try:
        df = pd.read_csv(file_path)
        if df.empty:
            return set()
        df['url'] = df['url'].fillna('N/A')
        return set(zip(df['content'], df['url']))
    except pd.errors.EmptyDataError:
        return set()
    except Exception as e:
        logging.warning(f"âš ï¸ {file_path} ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return set()

def save_to_csv(all_leaks):
    """íƒì§€ëœ ëª¨ë“  ë‚´ì—­ì„ 'ì˜ì‹¬' ëª©ë¡(CSV)ì— 'ì¶”ê°€'í•©ë‹ˆë‹¤."""
    # (ë‚´ìš© ë™ì¼ - ìƒëµ)
    if not all_leaks:
        return
            
    new_df = pd.DataFrame(all_leaks)
    new_df['url'] = new_df['url'].fillna('N/A')
    
    processed_keys = get_existing_keys(FEEDBACK_FILE)
    pending_keys = get_existing_keys(CSV_FILE)
    all_known_keys = processed_keys.union(pending_keys)
    
    is_truly_new = new_df.apply(lambda row: (row['content'], row['url']) not in all_known_keys, axis=1)
    final_new_df = new_df[is_truly_new]
    
    if final_new_df.empty:
        logging.info("âœ… ìƒˆë¡œ ë°œê²¬ëœ 'ì˜ì‹¬' ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤. (ëª¨ë‘ ê¸°ì¡´ ëª©ë¡ì— ì¡´ì¬)")
        return

    logging.info(f"âœ¨ {len(final_new_df)}ê±´ì˜ 'ì§„ì§œ ì‹ ê·œ' ë‚´ì—­ì„ {CSV_FILE}ì— ì¶”ê°€í•©ë‹ˆë‹¤.")
    final_new_df.to_csv(CSV_FILE, mode='a', header=not os.path.exists(CSV_FILE), index=False, encoding='utf-8-sig')

# --- 7. ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    logging.info("ğŸ¤– 1. 'ì‹ ì…' ë´‡(Crawler) ì‘ë™ ì‹œì‘...")
    
    logging.info("ğŸ§  ë´‡ì˜ AI ë‡Œ(NER ëª¨ë¸)ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...")
    ner_brain = load_ner_pipeline()
    if ner_brain is None:
        logging.error("âŒ AI ë‡Œ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ 'ì‹ ì…' ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit()
    logging.info("ğŸ§  AI ë‡Œ ë¡œë“œ ì™„ë£Œ.")

    # (âœ¨ ì‹ ê·œ) Selenium ë“œë¼ì´ë²„ 1íšŒ ì‹¤í–‰
    logging.info("ğŸš— Selenium (Chrome) ë“œë¼ì´ë²„ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...")
    driver = setup_selenium_driver()
    if driver is None:
        logging.error("âŒ Selenium ë“œë¼ì´ë²„ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ 'ì‹ ì…' ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit()
    logging.info("ğŸš— Selenium ë“œë¼ì´ë²„ ë¡œë“œ ì™„ë£Œ.")
    
    total_leaks_found = []
    
    # --- Seleniumì„ ì‚¬ìš©í•œ ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ---
    logging.info(f"ğŸ›°ï¸ [Selenium í¬ë¡¤ë§] {len(CRAWL_URLS)}ê°œì˜ URLì„ ìŠ¤ìº”í•©ë‹ˆë‹¤. (OCR ë¹„í™œì„±í™”)")
    for url in CRAWL_URLS:
        # (âœ¨ ìˆ˜ì •) driverë¥¼ ì¸ìë¡œ ì „ë‹¬
        leaks = crawl_web_page(url, ner_brain, driver) 
        for leak in leaks:
            leak['url'] = url 
            leak['repo'] = 'web-crawl' 
        total_leaks_found.extend(leaks)
        # (time.sleep(1)ì€ Seleniumì˜ 5ì´ˆ ëŒ€ê¸°ë¡œ ëŒ€ì²´ë˜ì—ˆìœ¼ë¯€ë¡œ ì œê±° ê°€ëŠ¥)

    # (âœ¨ ì‹ ê·œ) ë“œë¼ì´ë²„ ì¢…ë£Œ
    driver.quit()
    logging.info("ğŸš— Selenium ë“œë¼ì´ë²„ ì¢…ë£Œ ì™„ë£Œ.")

    # (ê¹ƒí—ˆë¸Œ API ê²€ìƒ‰ì€ ì—¬ì „íˆ ì£¼ì„ ì²˜ë¦¬)
            
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    if total_leaks_found:
        save_to_csv(total_leaks_found)
    
    logging.info("ğŸ¤– 1. 'ì‹ ì…' ë´‡(Crawler) ì‘ë™ ì™„ë£Œ.")
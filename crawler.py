# ğŸ•µï¸ (ë´‡ 1) 'ì‹ ì…' ë´‡. 'ì˜ì‹¬' ë‚´ì—­ ìˆ˜ì§‘ -> detected_leaks.csv
# ----------------------------------------------------
# (âœ¨ ìµœì¢… ë¡œì§)
# 1. í…ìŠ¤íŠ¸/ì´ë¯¸ì§€/GitHubì—ì„œ 'ì˜ì‹¬' PIIë¥¼ 1ì°¨ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
# 2. 'detected_leaks.csv' (In_1)ì™€ 'feedback_data.csv' (In_2)ë¥¼ ëª¨ë‘ í™•ì¸í•©ë‹ˆë‹¤.
# 3. ë‘ ê³³ ì–´ë””ì—ë„ ì—†ëŠ” "ì§„ì§œ ìƒˆë¡œìš´" í•­ëª©ë§Œ 'detected_leaks.csv' (In_1)ì— ì¶”ê°€í•©ë‹ˆë‹¤.
# ----------------------------------------------------

import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import os
import time
from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification
from urllib.parse import urljoin

# ìš°ë¦¬ í—¬í¼ ë° ì„¤ì • íŒŒì¼ ì„í¬íŠ¸
import config
import ocr_helper 

# --- 1. ì„¤ì •ê°’ ---
BASE_PATH = "/root/PII-Guardian" # (ì¤‘ìš”) deploy.ymlì˜ DEPLOY_DIRê³¼ ì¼ì¹˜
CSV_FILE = os.path.join(BASE_PATH, 'detected_leaks.csv')
FEEDBACK_FILE = os.path.join(BASE_PATH, 'feedback_data.csv')
MODEL_PATH = os.path.join(BASE_PATH, 'my-ner-model')
BASE_MODEL = 'klue/roberta-base' # ğŸ§  ê¸°ë³¸ ë‡Œ (Hugging Face)

REGEX_PATTERNS = {
    'EMAIL': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
    'PHONE': r'\b010[-.\s]?\d{4}[-.\s]?\d{4}\b',
}

# (âœ¨ í•µì‹¬ ìˆ˜ì •) 
# ì›¹ URL ëŒ€ì‹ , ì„œë²„ ë¡œì»¬ì˜ 'test_site' í´ë”ë¥¼ ì§ì ‘ ì½ìŠµë‹ˆë‹¤.
TEST_FILES = [
    os.path.join(BASE_PATH, 'test_site/index.html'),
    os.path.join(BASE_PATH, 'test_site/page_with_image.html')
]

# (âœ¨ ì£¼ì„ ì²˜ë¦¬) 
# GITHUB_QUERIES = [
#     '"ncp_api_key"',
#     '"IMë±…í¬" "ë¹„ë°€ë²ˆí˜¸"',
# ]

# --- 2. ë´‡ì˜ 'ë‡Œ' (AI ëª¨ë¸) ë¡œë“œ (âœ¨ ìµœì¢… ìˆ˜ì •) ---
def load_ner_pipeline():
    """ë´‡ì˜ 'ë‡Œ'(NER ëª¨ë¸)ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    
    # deploy.ymlì´ ìƒì„±í•œ í† í° 'íŒŒì¼'ì„ ì§ì ‘ ì½ì–´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    token_file_path = "/root/.cache/huggingface/token"
    hf_token = None
    if os.path.exists(token_file_path):
        try:
            with open(token_file_path, 'r') as f:
                hf_token = f.read().strip()
            if hf_token:
                 print("âœ… Hugging Face í† í° íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ì½ì—ˆìŠµë‹ˆë‹¤.")
            else:
                 print("âš ï¸ [ê²½ê³ ] /root/.cache/huggingface/token íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                 print("âš ï¸ GitHub Secret 'HF_TOKEN'ì— ê°’ì´ ì˜¬ë°”ë¥´ê²Œ ì…ë ¥ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”!")
        except Exception as e:
            print(f"âš ï¸ [ê²½ê³ ] Hugging Face í† í° íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    else:
        print("âš ï¸ [ê²½ê³ ] Hugging Face í† í° íŒŒì¼(/root/.cache/huggingface/token)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # (ì°¨ì„ ì±…) íŒŒì¼ì´ ì—†ì„ ê²½ìš°, config.pyì—ì„œë„ ì‹œë„
    if not hf_token:
        hf_token = getattr(config, 'HF_TOKEN', None)
        if hf_token:
             print("âœ… config.pyì—ì„œ HF_TOKENì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    
    if not hf_token:
        print("âŒ [ì¹˜ëª…ì  ì˜¤ë¥˜] Hugging Face í† í°ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None 

    try:
        # 1ìˆœìœ„: ìš°ë¦¬ê°€ í•™ìŠµì‹œí‚¨ 'ê²½ë ¥ì§' ë‡Œ(my-ner-model)ë¥¼ ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, token=hf_token)
        model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH, token=hf_token)
        print(f"âœ… 'ê²½ë ¥ì§' AI ë‡Œ({MODEL_PATH}) ë¡œë“œ ì„±ê³µ!")
        
    # (âœ¨ í•µì‹¬ ìˆ˜ì •) 
    # except OSError: -> except Exception:
    # 'ê²½ë ¥ì§' ë‡Œ ë¡œë“œì— "ì–´ë–¤ ì´ìœ ë¡œë“ " (OSError, ValueError ë“±) ì‹¤íŒ¨í•˜ë©´
    # 'ì‹ ì…' ë‡Œë¥¼ ë¡œë“œí•˜ë„ë¡ í•©ë‹ˆë‹¤.
    except Exception as e: 
        print(f"âš ï¸ 'ê²½ë ¥ì§' AI ë‡Œ({MODEL_PATH}) ë¡œë“œ ì‹¤íŒ¨. ì›ì¸: {e}")
        print(f"â¡ï¸ 'ì‹ ì…' ë‡Œ({BASE_MODEL})ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
        
        # 2ìˆœìœ„: 'ì‹ ì…' ë‡Œ(BASE_MODEL)ë¥¼ ë¡œë“œ
        try:
            tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, token=hf_token)
            model = AutoModelForTokenClassification.from_pretrained(BASE_MODEL, token=hf_token)
        except Exception as e2:
            print(f"âŒ [ì¹˜ëª…ì  ì˜¤ë¥˜] 'ì‹ ì…' ë‡Œ({BASE_MODEL}) ë¡œë“œì—ë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e2}")
            return None
        
    # AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ì‰½ê²Œ 'íŒŒì´í”„ë¼ì¸'ìœ¼ë¡œ ë§Œë“¦
    ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, device=-1, aggregation_strategy="simple")
    return ner_pipeline

# --- 3. ìœ ì¶œ íƒì§€ í•¨ìˆ˜ (í…ìŠ¤íŠ¸ìš©) ---
def find_leaks_in_text(text, ner_pipeline):
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ RegExì™€ NERë¡œ PIIë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    leaks = []
    
    context_preview = text.strip().replace('\n', ' ').replace('\r', ' ')[0:300]
    
    # 1. ì •ê·œì‹(RegEx)ìœ¼ë¡œ ë¨¼ì € íƒì§€
    for pii_type, pattern in REGEX_PATTERNS.items():
        for match in re.finditer(pattern, text):
            leaks.append({
                'type': pii_type,
                'content': match.group(0),
                'context': context_preview
            })
            
    # 2. AI(NER)ë¡œ ì¶”ê°€ íƒì§€
    try:
        ner_results = ner_pipeline(text[:512]) 
        for entity in ner_results:
            # klue/roberta-baseëŠ” 'PS'(ì‚¬ëŒ), 'LC'(ì¥ì†Œ), 'OG'(ê¸°ê´€) ë“±ì„ íƒì§€
            if entity['entity_group'] in ['PS', 'LC', 'OG']:
                leak_type = entity['entity_group']
                # 'PS' -> 'PERSON (AI)'ì²˜ëŸ¼ ì¢€ ë” ì¹œì ˆí•˜ê²Œ ë³€ê²½
                if leak_type == 'PS': leak_type = 'PERSON (AI)'
                if leak_type == 'LC': leak_type = 'LOCATION (AI)'
                if leak_type == 'OG': leak_type = 'ORGANIZATION (AI)'
                
                leaks.append({
                    'type': leak_type,
                    'content': entity['word'],
                    'context': context_preview
                })
    except Exception as e:
        print(f"âŒ [AI ë¶„ì„ ì—ëŸ¬] {e}")
            
    return leaks

# --- 4. í¬ë¡¤ë§ í•¨ìˆ˜ (âœ¨ ë¡œì»¬ íŒŒì¼ ì½ê¸°) ---
def crawl_local_file(file_path, ner_pipeline):
    """(ê¸°ëŠ¥ 1) í•˜ë‚˜ì˜ 'ë¡œì»¬ í…ŒìŠ¤íŠ¸ íŒŒì¼'ì„ ì½ìŠµë‹ˆë‹¤."""
    print(f"ğŸ•µï¸ [ë¡œì»¬ í…ŒìŠ¤íŠ¸] íŒŒì¼ ì½ê¸° ì‹œì‘: {file_path}")
    leaks_found = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
            
        soup = BeautifulSoup(html_content, 'html.parser')
        
        if not soup.body: return []
        page_text = soup.body.get_text(separator=' ')
        
        # 4-1. í…ìŠ¤íŠ¸
        leaks_found.extend(find_leaks_in_text(page_text, ner_pipeline))
        
        # 4-2. (âœ¨ ì£¼ì„ ì²˜ë¦¬) OCR ê¸°ëŠ¥
        # print("ğŸ–¼ï¸  ì´ë¯¸ì§€ ìŠ¤ìº” ê¸°ëŠ¥ì„ ì£¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
        return leaks_found
    except FileNotFoundError:
        print(f"âŒ [ì—ëŸ¬] {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    except Exception as e:
        print(f"âŒ [ì—ëŸ¬] {file_path} íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return []

# --- 5. (âœ¨ ì£¼ì„ ì²˜ë¦¬) ê¹ƒí—ˆë¸Œ ê²€ìƒ‰ í•¨ìˆ˜ ---
# def search_github_api(query, ner_pipeline): ...

# --- 6. CSV ì €ì¥ í•¨ìˆ˜ ---
def get_existing_keys(file_path):
    """CSV íŒŒì¼ì—ì„œ (content, url) í‚¤ ì„¸íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not os.path.exists(file_path):
        return set()
    try:
        df = pd.read_csv(file_path)
        if df.empty:
            return set()
        df['url'] = df['url'].fillna('N/A') # URL ì—†ëŠ” ê²½ìš° ëŒ€ë¹„
        return set(zip(df['content'], df['url']))
    except pd.errors.EmptyDataError:
        return set()
    except Exception as e:
        print(f"âš ï¸ {file_path} ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return set()

def save_to_csv(all_leaks):
    """íƒì§€ëœ ëª¨ë“  ë‚´ì—­ì„ 'ì˜ì‹¬' ëª©ë¡(CSV)ì— 'ì¶”ê°€'í•©ë‹ˆë‹¤."""
    if not all_leaks:
        return
            
    new_df = pd.DataFrame(all_leaks)
    new_df['url'] = new_df['url'].fillna('N/A')
    
    # (âœ¨ í•µì‹¬ 1) ì´ë¯¸ ì²˜ë¦¬ëœ 'ì •ë‹µ' ëª©ë¡(feedback)ì— ìˆëŠ”ì§€ í™•ì¸
    processed_keys = get_existing_keys(FEEDBACK_FILE)
    
    # (âœ¨ í•µì‹¬ 2) ì´ë¯¸ 'ì˜ì‹¬' ëª©ë¡(detected)ì— ìˆëŠ”ì§€ í™•ì¸
    pending_keys = get_existing_keys(CSV_FILE)
    
    # (âœ¨ í•µì‹¬ 3) ë‘ ê³³ ëª¨ë‘ì— ì—†ëŠ” "ì§„ì§œ ìƒˆë¡œìš´" í•­ëª©ë§Œ í•„í„°ë§
    all_known_keys = processed_keys.union(pending_keys)
    
    is_truly_new = new_df.apply(lambda row: (row['content'], row['url']) not in all_known_keys, axis=1)
    final_new_df = new_df[is_truly_new]
    
    if final_new_df.empty:
        print("âœ… ìƒˆë¡œ ë°œê²¬ëœ 'ì˜ì‹¬' ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤. (ëª¨ë‘ ê¸°ì¡´ ëª©ë¡ì— ì¡´ì¬)")
        return

    # (âœ¨ í•µì‹¬ 4) "ì§„ì§œ ìƒˆë¡œìš´" í•­ëª©ë§Œ 'ì˜ì‹¬' ëª©ë¡(detected_leaks.csv)ì— 'ì¶”ê°€'
    print(f"âœ¨ {len(final_new_df)}ê±´ì˜ 'ì§„ì§œ ì‹ ê·œ' ë‚´ì—­ì„ {CSV_FILE}ì— ì¶”ê°€í•©ë‹ˆë‹¤.")
    final_new_df.to_csv(CSV_FILE, mode='a', header=not os.path.exists(CSV_FILE), index=False, encoding='utf-8-sig')

# --- 7. ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    print("ğŸ¤– 1. 'ì‹ ì…' ë´‡(Crawler) ì‘ë™ ì‹œì‘...")
    
    print("ğŸ§  ë´‡ì˜ AI ë‡Œ(NER ëª¨ë¸)ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...")
    ner_brain = load_ner_pipeline()
    
    if ner_brain is None:
        print("âŒ AI ë‡Œ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ 'ì‹ ì…' ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit() # ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ
        
    print("ğŸ§  AI ë‡Œ ë¡œë“œ ì™„ë£Œ.")
    
    total_leaks_found = []
    
    # (âœ¨ ìˆ˜ì •) ë¡œì»¬ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°
    for file_path in TEST_FILES:
        leaks = crawl_local_file(file_path, ner_brain)
        for leak in leaks:
            leak['url'] = os.path.basename(file_path) # url ëŒ€ì‹  íŒŒì¼ëª…(index.html) ê¸°ë¡
            leak['repo'] = 'test-site'
        total_leaks_found.extend(leaks)
        
    # (âœ¨ ì£¼ì„ ì²˜ë¦¬) GitHub API ê²€ìƒ‰
    # print("ğŸ›°ï¸ [GitHub API] ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    if total_leaks_found:
        save_to_csv(total_leaks_found)
    
    print("ğŸ¤– 1. 'ì‹ ì…' ë´‡(Crawler) ì‘ë™ ì™„ë£Œ.")
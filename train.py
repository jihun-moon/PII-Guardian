# ğŸ“ (ë´‡ 3) 'í•™ìŠµê¸°' ë´‡. 'ìë™' ì •ë‹µìœ¼ë¡œ 'ì‹ ì…' ë´‡ ë‡Œ í›ˆë ¨ -> my-ner-model
# (v2.0 - ì‹¤ì œ Fine-Tuning ë²„ì „)
# ----------------------------------------------------
# 1. 'ì •ë‹µ' ëª©ë¡ (feedback_data.csv) [In_2]ë¥¼ ì½ìŠµë‹ˆë‹¤.
# 2. 'trained.log' (í•™ìŠµ ê¸°ë¡)ì„ ì½ìŠµë‹ˆë‹¤.
# 3. [In_2]ì—ë§Œ ìˆê³  [í•™ìŠµ ê¸°ë¡]ì—ëŠ” ì—†ëŠ” "ìƒˆë¡œìš´ ì •ë‹µ"ë§Œ í•™ìŠµí•©ë‹ˆë‹¤.
# 4. í•™ìŠµ ì™„ë£Œ í›„, "ìƒˆë¡œìš´ ì •ë‹µ"ì˜ IDë¥¼ 'trained.log'ì— 'ì¶”ê°€'í•©ë‹ˆë‹¤.
# 5. (âœ¨ ì‹ ê·œ) ì¬í•™ìŠµëœ 'ê²½ë ¥ì§' ë‡Œë¥¼ 'my-ner-model' í´ë”ì— ì €ì¥í•©ë‹ˆë‹¤.
# ----------------------------------------------------

import pandas as pd
import os
import datetime
import logging
import config # (âœ¨ ì‹ ê·œ) HF_TOKENì„ ì½ê¸° ìœ„í•´
from datasets import Dataset # (âœ¨ ì‹ ê·œ)
from transformers import ( # (âœ¨ ì‹ ê·œ)
    AutoTokenizer,
    AutoModelForTokenClassification,
    TrainingArguments,
    Trainer
)

# --- 1. ì„¤ì •ê°’ ---
BASE_PATH = "/root/PII-Guardian" 
LOG_FILE = os.path.join(BASE_PATH, 'train.log')

# (âœ¨âœ¨âœ¨ í•µì‹¬ ìˆ˜ì •: ë¡œê·¸ ì¤‘ë³µ ì œê±° âœ¨âœ¨âœ¨)
# FileHandlerë¥¼ ì œê±°í•˜ê³  StreamHandlerë§Œ ë‚¨ê¹ë‹ˆë‹¤.
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.StreamHandler()])

FEEDBACK_FILE = os.path.join(BASE_PATH, 'feedback_data.csv')
MODEL_PATH = os.path.join(BASE_PATH, 'my-ner-model') # ğŸ§  'ê²½ë ¥ì§' ë‡Œ ì €ì¥ ê²½ë¡œ
TRAINED_LOG_FILE = os.path.join(BASE_PATH, 'trained.log')
BASE_MODEL = 'klue/roberta-base' # ğŸ§  'ì‹ ì…' ë‡Œ (ê¸°ë³¸ ëª¨ë¸)

# (âœ¨ ì‹ ê·œ) NER íƒœê·¸ ì •ì˜ (IOB2 í˜•ì‹)
# O = Outside (PII ì•„ë‹˜)
# B-PII = Beginning (PII ì‹œì‘)
# I-PII = Inside (PII ì¤‘ê°„/ë)
label_list = ['O', 'B-PII', 'I-PII']
label2id = {label: i for i, label in enumerate(label_list)}
id2label = {i: label for i, label in enumerate(label_list)}


# --- 2. ë¡œê·¸ ê´€ë¦¬ í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼) ---
def load_trained_log():
    """ì´ë¯¸ í•™ìŠµí•œ í•­ëª©(ì¤‘ë³µ í•™ìŠµ ë°©ì§€ìš©)ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    if not os.path.exists(TRAINED_LOG_FILE):
        return set()
    try:
        with open(TRAINED_LOG_FILE, 'r', encoding='utf-8') as f:
            return set(line.strip() for line in f)
    except Exception as e:
        logging.warning(f"âš ï¸ 'trained.log' ë¡œë“œ ì‹¤íŒ¨: {e}")
        return set()

def save_trained_log(unique_id):
    """í•™ìŠµ ì™„ë£Œëœ í•­ëª©ì„ ê¸°ë¡í•©ë‹ˆë‹¤."""
    with open(TRAINED_LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(unique_id + '\n')

# --- 3. (âœ¨ ì‹ ê·œ) ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ---
def preprocess_for_ner(new_data_df, tokenizer):
    """
    (context, content) ë°ì´í„°ë¥¼ NER í•™ìŠµìš© IOB2 íƒœê·¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    dataset_list = []
    
    for index, row in new_data_df.iterrows():
        context = str(row.get('context', ''))
        content = str(row.get('content', ''))

        if not context or not content:
            continue
            
        # 1. PII (content)ê°€ ë¬¸ë§¥(context) ì–´ë””ì— ìˆëŠ”ì§€ ì°¾ê¸°
        start_idx = context.find(content)
        if start_idx == -1:
            logging.warning(f"âš ï¸ í•™ìŠµ ë°ì´í„° ì˜¤ë¥˜: PII '{content}'ê°€ ë¬¸ë§¥ '{context[:50]}...'ì— ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
        end_idx = start_idx + len(content)

        # 2. ë¬¸ë§¥(context)ì„ í† í¬ë‚˜ì´ì €ë¡œ ë¶„ì ˆ
        tokenized_inputs = tokenizer(context, truncation=True, max_length=512, return_offsets_mapping=True)
        offsets = tokenized_inputs.pop("offset_mapping")
        
        # 3. ëª¨ë“  í† í°ì„ 'O' (PII ì•„ë‹˜)ìœ¼ë¡œ ì´ˆê¸°í™”
        labels = [label2id['O']] * len(tokenized_inputs['input_ids'])
        is_b_token = True # 'B-PII' íƒœê·¸ë¥¼ ë¶™ì˜€ëŠ”ì§€ í™•ì¸

        # 4. í† í°ì˜ ìœ„ì¹˜(offset)ì™€ PII ìœ„ì¹˜(start_idx, end_idx)ë¥¼ ë¹„êµ
        for i, (offset_start, offset_end) in enumerate(offsets):
            # (ì˜ˆì™¸ ì²˜ë¦¬) [CLS], [SEP] ê°™ì€ íŠ¹ìˆ˜ í† í°
            if offset_start == 0 and offset_end == 0:
                labels[i] = -100 # loss ê³„ì‚°ì—ì„œ ì œì™¸
                continue

            # (í•µì‹¬) í˜„ì¬ í† í°ì´ PII ë²”ìœ„ ì•ˆì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
            if offset_start >= start_idx and offset_end <= end_idx:
                if is_b_token:
                    labels[i] = label2id['B-PII'] # ì²« í† í°ì€ B-PII
                    is_b_token = False
                else:
                    labels[i] = label2id['I-PII'] # ë‚˜ë¨¸ì§€ëŠ” I-PII
            else:
                is_b_token = True # PII ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ B-PII ì´ˆê¸°í™”
        
        tokenized_inputs['labels'] = labels
        dataset_list.append(tokenized_inputs)
        
    if not dataset_list:
        return None
        
    return Dataset.from_list(dataset_list)

# --- 4. ë©”ì¸ ì‹¤í–‰ ---
def main():
    logging.info("ğŸ¤– 3. 'í•™ìŠµê¸°' ë´‡(Trainer) ì‘ë™ ì‹œì‘...")
    
    # 1. 'ì •ë‹µ' íŒŒì¼(In_2) ë¡œë“œ
    if not os.path.exists(FEEDBACK_FILE):
        logging.warning(f"âš ï¸ 'ì •ë‹µ' ëª©ë¡({FEEDBACK_FILE})ì´ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    try:
        feedback_df = pd.read_csv(FEEDBACK_FILE)
        feedback_df['url'] = feedback_df['url'].fillna('N/A')
    except pd.errors.EmptyDataError:
        logging.info("âœ… 'ì •ë‹µ' ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    except Exception as e:
        logging.error(f"âŒ 'ì •ë‹µ' íŒŒì¼ ë¡œë“œ ì¤‘ ì—ëŸ¬: {e}")
        return
        
    # 2. 'í•™ìŠµ ê¸°ë¡' ë¡œë“œ ë° 'ìƒˆë¡œìš´ í•™ìŠµ ë°ì´í„°' í•„í„°ë§
    trained_set = load_trained_log()
    
    new_data_rows = []
    new_ids_to_log = []
    
    for index, row in feedback_df.iterrows():
        unique_id = f"{row['content']}|{row['url']}"
        # (ì¡°ê±´ 1) "ìœ ì¶œ" ë¼ë²¨ì´ê³ , (ì¡°ê±´ 2) "ì•„ì§ í•™ìŠµ ì•ˆ í•œ" ë°ì´í„°
        if row['llm_label'] == 'ìœ ì¶œ' and unique_id not in trained_set:
            new_data_rows.append(row.to_dict())
            new_ids_to_log.append(unique_id)

    if not new_data_rows:
        logging.info("âœ… ìƒˆë¡œ í•™ìŠµí•  'ìœ ì¶œ' ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ëª¨ë‘ ì´ì „ì— í•™ìŠµ ì™„ë£Œ)")
        return

    logging.info(f"ğŸ”¥ ì´ {len(new_data_rows)}ê°œì˜ 'ìƒˆë¡œìš´ ìœ ì¶œ' ìƒ˜í”Œë¡œ ë‡Œë¥¼ ì¬í•™ìŠµ(Fine-Tuning)í•©ë‹ˆë‹¤...")
    new_data_df = pd.DataFrame(new_data_rows)

    # 3. (âœ¨ ì‹ ê·œ) ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    HF_TOKEN = getattr(config, 'HF_TOKEN', None)
    if not HF_TOKEN:
        logging.error("âŒ config.pyì—ì„œ HF_TOKENì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return
        
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, token=HF_TOKEN)
    model = AutoModelForTokenClassification.from_pretrained(
        BASE_MODEL, 
        num_labels=len(label_list),
        id2label=id2label,
        label2id=label2id,
        token=HF_TOKEN
    )
    
    # 4. (âœ¨ ì‹ ê·œ) ë°ì´í„° ì „ì²˜ë¦¬
    logging.info("ë°ì´í„° ì „ì²˜ë¦¬(NER íƒœê¹…) ì‹œì‘...")
    train_dataset = preprocess_for_ner(new_data_df, tokenizer)
    
    if train_dataset is None:
        logging.warning("âš ï¸ ì „ì²˜ë¦¬ í›„ í•™ìŠµí•  ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        # (ì°¸ê³ : PIIë¥¼ contextì—ì„œ ëª» ì°¾ëŠ” ë“±ì˜ ì´ìœ ë¡œ ë°ì´í„°ê°€ 0ì´ ë  ìˆ˜ ìˆìŒ)
        return
        
    logging.info(f"âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ. (ìœ íš¨ ìƒ˜í”Œ: {len(train_dataset)})")

    # 5. (âœ¨ ì‹ ê·œ) ì‹¤ì œ í•™ìŠµ(Fine-Tuning) ì‹œì‘
    # (time.sleep(30)ì„ ì‹¤ì œ ì½”ë“œë¡œ ëŒ€ì²´)
    
    # (NCP ì„œë²„ ì‚¬ì–‘ì— ë§ì¶° ìµœì†Œí•œì˜ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ)
    training_args = TrainingArguments(
        output_dir=os.path.join(MODEL_PATH, "checkpoints"), # í•™ìŠµ ì¤‘ê°„ ê³¼ì • ì €ì¥
        num_train_epochs=3,             # 3ë²ˆ ë°˜ë³µ í•™ìŠµ
        per_device_train_batch_size=2,  # í•œ ë²ˆì— 2ê°œì”© (CPU/ì €ì‚¬ì–‘ GPUìš©)
        save_strategy="epoch",          # 1 ì—í¬í¬ë§ˆë‹¤ ì €ì¥
        logging_steps=10,               # 10 ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
        report_to="none"                # (í•„ìˆ˜) wandb ê°™ì€ ì™¸ë¶€ ë¡œê¹… ë¹„í™œì„±í™”
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset
        # (í‰ê°€ ë°ì´í„°ì…‹ì€ ìƒëµ)
    )

    logging.info("ğŸ”¥ 'ê²½ë ¥ì§' ë‡Œ ì‹¤ì œ í•™ìŠµ ì‹œì‘... (CPU/GPU ì‚¬ìš©)")
    trainer.train()
    logging.info("âœ… ì¬í•™ìŠµ ì™„ë£Œ!")

    # 6. (âœ¨ ì‹ ê·œ) 'ê²½ë ¥ì§' ë‡Œ ìµœì¢… ì €ì¥
    logging.info(f"ğŸ’¾ 'ê²½ë ¥ì§' ë‡Œë¥¼ {MODEL_PATH}ì— ì €ì¥í•©ë‹ˆë‹¤.")
    trainer.save_model(MODEL_PATH)
    tokenizer.save_pretrained(MODEL_PATH) # (ì¤‘ìš”) í† í¬ë‚˜ì´ì €ë„ í•¨ê»˜ ì €ì¥

    # 7. (ê¸°ì¡´) "í•™ìŠµ ì™„ë£Œ"ëœ IDë“¤ì„ ë¡œê·¸ì— ê¸°ë¡ (ì¤‘ë³µ í•™ìŠµ ë°©ì§€)
    for unique_id in new_ids_to_log:
        save_trained_log(unique_id)
        
    logging.info(f"ğŸ’¾ {len(new_ids_to_log)}ê±´ì„ 'í•™ìŠµ ì™„ë£Œ' ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
    logging.info("ğŸ¤– 3. 'í•™ìŠµê¸°' ë´‡(Trainer) ì‘ë™ ì™„ë£Œ.")

if __name__ == "__main__":
    main()